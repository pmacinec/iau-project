{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAU - Project\n",
    "\n",
    "**Authors:** Peter Mačinec, Lukáš Janík"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import metrics\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data\n",
    "\n",
    "Data are divided into two files, personal and other, so we need to read both of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "df1 = pd.read_csv('data/personal_train.csv', index_col=0)\n",
    "df2 = pd.read_csv('data/other_train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge datasets\n",
    "\n",
    "At first, we need to merge both datasets into one. In previous analysis, we found that name and address would be used for merging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df1, df2, on=[\"name\", \"address\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In descriptive analysis, we found some duplicates. In second dataset with medical information, there were some duplicates, so we will merge their values and drop duplicated rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data repairing\n",
    "\n",
    "We know from a previous analysis that some data need to be repaired. Some columns have one value represented by more strings, another case is that column holds several values that need to be expanded, etc. In this section, data will be repaired at first so missing values would be replaced in next step.\n",
    "\n",
    "All operations will be done using **Pipelines**, so whole preprocessing process will be reusable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge and drop duplicates\n",
    "\n",
    "As mentioned before, there are some duplicates. Let's check them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df_train[df_train.duplicated(['name', 'address'], keep='first')].sort_values('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>query hyperthyroid</th>\n",
       "      <th>T4U measured</th>\n",
       "      <th>FTI measured</th>\n",
       "      <th>lithium</th>\n",
       "      <th>TT4</th>\n",
       "      <th>...</th>\n",
       "      <th>personal_info</th>\n",
       "      <th>T3 measured</th>\n",
       "      <th>on antithyroid medication</th>\n",
       "      <th>referral source</th>\n",
       "      <th>education-num</th>\n",
       "      <th>psych</th>\n",
       "      <th>occupation</th>\n",
       "      <th>TBG measured</th>\n",
       "      <th>TBG</th>\n",
       "      <th>pregnant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>Alfred Still</td>\n",
       "      <td>4175 Smith Keys\\nNew Taylor, NH 39815</td>\n",
       "      <td>57.0</td>\n",
       "      <td>M</td>\n",
       "      <td>1960-11-02</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>82.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>other</td>\n",
       "      <td>13.0</td>\n",
       "      <td>f</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>Amelia Rodriguez</td>\n",
       "      <td>087 Gary Port\\nWest Sarah, KY 66896</td>\n",
       "      <td>77.0</td>\n",
       "      <td>F</td>\n",
       "      <td>1941-03-17</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>White|United-States\\nBachelors -- Widowed|Unma...</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>SVI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>Sales</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>Angela Boyer</td>\n",
       "      <td>3750 Chen Groves\\nPamelatown, ME 02894</td>\n",
       "      <td>75.0</td>\n",
       "      <td>F</td>\n",
       "      <td>1942-12-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>White|United-States\\nHS-grad -- Divorced|Own-c...</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>SVI</td>\n",
       "      <td>9.0</td>\n",
       "      <td>f</td>\n",
       "      <td>Priv-house-serv</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>Anna Garcia</td>\n",
       "      <td>71052 Annette Roads\\nChristinechester, MT 16249</td>\n",
       "      <td>65.0</td>\n",
       "      <td>F</td>\n",
       "      <td>1953-05-06</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>White|United-States\\nHS-grad -- Never-married|...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>f</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>Annette Hunt</td>\n",
       "      <td>USNV Lamb\\nFPO AA 85130</td>\n",
       "      <td>33.0</td>\n",
       "      <td>F</td>\n",
       "      <td>1984-12-08</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>White|United-States\\nSome-college -- Married-c...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>f</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name                                          address   age  \\\n",
       "1656      Alfred Still            4175 Smith Keys\\nNew Taylor, NH 39815  57.0   \n",
       "855   Amelia Rodriguez              087 Gary Port\\nWest Sarah, KY 66896  77.0   \n",
       "904       Angela Boyer           3750 Chen Groves\\nPamelatown, ME 02894  75.0   \n",
       "1597       Anna Garcia  71052 Annette Roads\\nChristinechester, MT 16249  65.0   \n",
       "2204      Annette Hunt                          USNV Lamb\\nFPO AA 85130  33.0   \n",
       "\n",
       "     sex date_of_birth query hyperthyroid T4U measured FTI measured lithium  \\\n",
       "1656   M    1960-11-02                  f            t            t       f   \n",
       "855    F    1941-03-17                  f          NaN            t       f   \n",
       "904    F    1942-12-28                NaN          NaN            t       f   \n",
       "1597   F    1953-05-06                  f            f            f       f   \n",
       "2204   F    1984-12-08                  f            f            f       f   \n",
       "\n",
       "       TT4   ...                                         personal_info  \\\n",
       "1656  82.0   ...                                                   NaN   \n",
       "855   84.0   ...     White|United-States\\nBachelors -- Widowed|Unma...   \n",
       "904   92.0   ...     White|United-States\\nHS-grad -- Divorced|Own-c...   \n",
       "1597   NaN   ...     White|United-States\\nHS-grad -- Never-married|...   \n",
       "2204   NaN   ...     White|United-States\\nSome-college -- Married-c...   \n",
       "\n",
       "      T3 measured  on antithyroid medication referral source education-num  \\\n",
       "1656            f                          f           other          13.0   \n",
       "855             t                          f             SVI           NaN   \n",
       "904             t                          f             SVI           9.0   \n",
       "1597            f                          f             NaN           9.0   \n",
       "2204            f                          f             NaN          10.0   \n",
       "\n",
       "     psych          occupation  TBG measured  TBG pregnant  \n",
       "1656     f      Prof-specialty             f    ?        f  \n",
       "855      f               Sales             f    ?        f  \n",
       "904      f     Priv-house-serv             f    ?        f  \n",
       "1597     f   Handlers-cleaners             f    ?      NaN  \n",
       "2204     f        Adm-clerical             f    ?        f  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are duplicates with same name and address, but they are even not representing different medical records (measurements are the same). In some attributes, one of duplicates has value and in the other one is this value missing. That means we need to merge those records before droping duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duplicated = df_train[df_train.duplicated(['name', 'address'], keep=false)]\n",
    "duplicate_names = df_train[df_train.duplicated(['name', 'address'], keep='first')].name.values\n",
    "\n",
    "for name in duplicate_names:\n",
    "    duplicates = duplicated[duplicated['name'] == name]\n",
    "    \n",
    "\n",
    "df_train =  df_train[~df_train['name'].isin(duplicate_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(vstup):\n",
    "    return reduce(lambda x,y: x if not pd.isna(x) else y, vstup)\n",
    "\n",
    "\n",
    "def deduplicate(df,columns = []):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    deduplicated = df_copy[df_copy.duplicated(subset=columns, keep=False)].groupby(columns).agg(func).reset_index()\n",
    "    \n",
    "    df_copy.drop_duplicates(subset=columns, keep=False, inplace=True)\n",
    "    \n",
    "    return pd.concat([df_copy,deduplicated], sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func({1, None, None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeRemoveDuplicates(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def func(vstup):\n",
    "        return reduce(lambda x,y: x if not pd.isna(x) else y, vstup)\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        duplicated = df[df.duplicated(['name', 'address'], keep=false)]\n",
    "        duplicate_names = df[df.duplicated(['name', 'address'], keep='first')].name.values\n",
    "        df =  df[~df['name'].isin(duplicate_names)]\n",
    "\n",
    "        return df.append(duplicated.groupby(['name', 'address']).agg(func).reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This class will be used for preprocessing in **Pipelines**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows with missing values in predicted attribute\n",
    "\n",
    "Rows where even value of predicted attribute is missing, will not help classifying in *supervised learning*. In this case, those values would be dropped. Let's check records with missing values for **class** attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Frank Gerace</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>Carol Crum</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>Cynthia Schmidtke</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>Don Carroll</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>Shirley Kiser</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>Lila Womack</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1840</th>\n",
       "      <td>Jane Little</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name class\n",
       "362        Frank Gerace   NaN\n",
       "575          Carol Crum   NaN\n",
       "1321  Cynthia Schmidtke   NaN\n",
       "1519        Don Carroll   NaN\n",
       "1675      Shirley Kiser   NaN\n",
       "1771        Lila Womack   NaN\n",
       "1840        Jane Little   NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['class'].isnull()][['name', 'class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this operation reusable, it is better to write custom pipeline with column as parameter, so every row with missing values in this column will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropRowsNanColumn(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df = df[pd.notnull(df[self.column])]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values unifying\n",
    "\n",
    "In some columns, missing values are represented by *nan*, or also by *'?'* character. Those values need to be unified, so we can fill them later using universal pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanUnifier(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df.loc[df[self.column].str.strip() == '?', self.column] = np.NaN\n",
    "        #df[self.column] = pd.to_numeric(df[self.column])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean unifying\n",
    "\n",
    "A lot of columns that store boolean values, mostly whether was measurement done or not, have inconsistent representation of boolean values (t, t.19, ...). It is better to unify them, because as it is categorical attribute, every reasonable value type should be represented just by one specific value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifyBoolean(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df_copy = df.copy()\n",
    "        df_copy[self.column] = df_copy[self.column].map(lambda x: str(x).lower().startswith('t'), na_action='ignore')\n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop useless columns\n",
    "\n",
    "Some columns will not help us in predicting class of the patient. It is because those column store only one value, as *TBG measured* attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['f', nan, 'f.14'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['TBG measured'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After unifying boolean values, it will contain only *false* value. It is better to drop columns like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropColumn(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "        \n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, **transform_params):\n",
    "        df = df.drop([self.column], axis=1)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expanding columns\n",
    "\n",
    "In analysis we found also column that store more attributes and their values. With this datatype, machine learning algorithms will not be able to work even though it can hold important information for prediction. We need to expand those objects into alone-standing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExpander(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df['bred'] = df['personal_info'].str.extract('(^[^|]+)', expand=False).str.strip().str.lower()\n",
    "        df['origin'] = df['personal_info'].str.extract('[|](.*)\\r', expand=False).str.strip().str.lower()\n",
    "        df['study'] = df['personal_info'].str.extract('[\\n](.*)--', expand=False).str.strip().str.lower()\n",
    "        df['status1'] = df['personal_info'].str.extract('--(.*)[|]', expand=False).str.strip().str.lower()\n",
    "        df['status2'] = df['personal_info'].str.extract('--.*[|](.*)', expand=False).str.strip().str.lower()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repair data using Pipeline\n",
    "\n",
    "For data repair, Pipeline will be used. All pipeline custom classes have already been defined in each section of **data repairing**, so we can just use them now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "repair_ppl = Pipeline([\n",
    "                # unify boolean values\n",
    "                ('ub01', UnifyBoolean('query hyperthyroid')),\n",
    "                ('ub02', UnifyBoolean('T4U measured')),\n",
    "                ('ub03', UnifyBoolean('on thyroxine')),\n",
    "                ('ub04', UnifyBoolean('FTI measured')),\n",
    "                ('ub05', UnifyBoolean('lithium')),\n",
    "                ('ub06', UnifyBoolean('TT4 measured')),\n",
    "                ('ub07', UnifyBoolean('query hypothyroid')),\n",
    "                ('ub08', UnifyBoolean('query on thyroxine')),\n",
    "                ('ub09', UnifyBoolean('tumor')),\n",
    "                ('ub10', UnifyBoolean('T3 measured')),\n",
    "                ('ub11', UnifyBoolean('sick')),\n",
    "                ('ub12', UnifyBoolean('thyroid surgery')),\n",
    "                ('ub13', UnifyBoolean('I131 treatment')),\n",
    "                ('ub14', UnifyBoolean('goitre')),\n",
    "                ('ub15', UnifyBoolean('TSH measured')),\n",
    "                ('ub16', UnifyBoolean('on antithyroid medication')),\n",
    "                ('ub17', UnifyBoolean('psych')),\n",
    "                ('ub18', UnifyBoolean('TBG measured')),\n",
    "                ('ub19', UnifyBoolean('pregnant')),\n",
    "                ('ub20', UnifyBoolean('hypopituitary')),\n",
    "\n",
    "                # drop column\n",
    "                ('drop_TBG_measured', DropColumn('TBG measured')),\n",
    "                ('drop_TBG', DropColumn('TBG')),\n",
    "\n",
    "                # expand column\n",
    "                ('expand_personal_info', ColumnExpander()),\n",
    "    \n",
    "                # unify nan values\n",
    "                ('nan_unify_FTI', NanUnifier('FTI')),\n",
    "                ('nan_unify_sex', NanUnifier('sex')),\n",
    "                ('nan_unify_occupation', NanUnifier('occupation')),\n",
    "\n",
    "                # drop, where are nan values\n",
    "                ('drop_class', DropRowsNanColumn('class')),\n",
    "    \n",
    "                # merge and remove duplicates\n",
    "                #('test',MergeRemoveDuplicates())\n",
    "\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = repair_ppl.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed.origin.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize and remove outliers\n",
    "\n",
    "In a lot of columns, mostly in those storing measurements values, outliers were found. For some algorithms, it is better to remove them or replace with quantiles. Before doing this, values should be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize numerical attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        _, self.lmbda = boxcox(df[self.column]+2)\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df_copy = df.copy()\n",
    "        df_copy[self.column] = boxcox(df_copy[self.column]+2, lmbda=attr)\n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutliersRemover(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        self.quantile_05 = df[self.column].quantile(.05)\n",
    "        self.quantiles_95 = df[self.column].quantile(.95)\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df.loc[df[self.column] > self.quantile_05, self.column] = self.quantile_05\n",
    "        df.loc[df[self.column] > self.quantile_95, self.column] = self.quantile_95\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_ppl = Pipeline([\n",
    "                    ('nieco', Normalizer('column')),\n",
    "                    ('nieco2', OutliersRemover('column'))\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = normalize_ppl.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values\n",
    "\n",
    "Our dataset contains also missing values (NaN), that should be filled before using them in machine learning algorithm. Missing values of numerical, and also categorical attributes should be filled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill numerical with median\n",
    "\n",
    "POPISAT IMPUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumMedianFiller(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        self.median = df[self.column].median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df_copy = df.copy()\n",
    "        df_copy.loc[df_copy[self.column].isnull(), self.column] = self.median\n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill numerical with Linear Regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumModelFiller(TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        self.model.fit(df)\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        self.model.predict(df)\n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill categorical with most frequent values\n",
    "\n",
    "POPISAT IMPUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalMostFrequentFiller(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "        \n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        self.most_frequent = df[self.column].value_counts().index[0]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, **transform_params):\n",
    "        df_copy = df.copy()\n",
    "        df_copy.loc[df_copy[self.column].isnull(), self.column] = self.most_frequent\n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill categorical with k-NN (k-nearest neighbours) algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalModelFiller(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill_ppl = Pipeline([\n",
    "#                 ('nieco', Imputer(strategy='median')),\n",
    "#                 ('nieco2', Imputer(strategy='most_frequent'))\n",
    "#     ])\n",
    "fill_ppl = Pipeline([\n",
    "                ('nieco', NumMedianFiller('T4U')),\n",
    "                ('nieco2', CategoricalMostFrequentFiller('sex'))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.sex.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fill_ppl.fit(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf2 = model.transform(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_model_ppl = Pipeline([\n",
    "                    ('nieco', NumModelFiller('column')),\n",
    "                    ('nieco2', CategoricalModelFiller('column'))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fill_ppl.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! porovnat ich ako pisali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = transformed[~transformed['FTI'].isnull()]\n",
    "columns = ['TT4', 'T4U','capital-loss', 'capital-gain', 'TSH', 'T3', 'fnlwgt', 'hours-per-week', 'education-num']\n",
    "\n",
    "for column in columns:\n",
    "    tmp.dropna(subset=[column], inplace=True)\n",
    "    \n",
    "X = tmp[columns]\n",
    "y = tmp['FTI']\n",
    "\n",
    "X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "train_preds = model.predict(X_test)\n",
    "metrics.mean_absolute_error(y_test, train_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "pca = TruncatedSVD(n_components=1)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pca.transform(X_train)\n",
    "test1 = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test1, y_test, color = 'red')\n",
    "plt.plot(train1, train_preds, color = 'blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO\n",
    "* urobit numeric co nie je numeric a malo by byt (FTI)\n",
    "* origin stlpec je empty, asi je expand zly\n",
    "* po oprave treba pozriet ci origin netreba tiez inak predspracovat\n",
    "* otestovat merge duplicates\n",
    "* po data repair ukazat, ze sme opravili hodnoty...\n",
    "* otestovat normalizer\n",
    "* otestovat outlier remover\n",
    "* urcit, ktore treba normalizovat a napisat do pipeliny\n",
    "* urcit, kde treba vymazat outlierov a napisat do pipeliny\n",
    "* okomentovat cast s normalizaciou a outliermi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
