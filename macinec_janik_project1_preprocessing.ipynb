{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# IAU - Project\n",
    "\n",
    "**Authors:** Peter Mačinec, Lukáš Janík"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import metrics\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Read the data\n",
    "\n",
    "Data are divided into two files, personal and other, so we need to read both of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# read datasets\n",
    "df1 = pd.read_csv('data/personal_train.csv', index_col=0)\n",
    "df2 = pd.read_csv('data/other_train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Merge datasets\n",
    "\n",
    "At first, we need to merge both datasets into one. In previous analysis, we found that name and address would be used for merging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df_train = pd.merge(df1, df2, on=[\"name\", \"address\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In descriptive analysis, we found some duplicates. In second dataset with medical information, there were some duplicates, so we will merge their values and drop duplicated rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data repairing\n",
    "\n",
    "We know from a previous analysis that some data need to be repaired. Some columns have one value represented by more strings, another case is that column holds several values that need to be expanded, etc. In this section, data will be repaired at first so missing values would be replaced in next step.\n",
    "\n",
    "All operations will be done using **Pipelines**, so whole preprocessing process will be reusable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Merge and drop duplicates\n",
    "\n",
    "As mentioned before, there are some duplicates. Let's check them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "duplicates = df_train[df_train.duplicated(['name', 'address'], keep='first')].sort_values('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>query hyperthyroid</th>\n",
       "      <th>T4U measured</th>\n",
       "      <th>FTI measured</th>\n",
       "      <th>lithium</th>\n",
       "      <th>TT4</th>\n",
       "      <th>...</th>\n",
       "      <th>personal_info</th>\n",
       "      <th>T3 measured</th>\n",
       "      <th>on antithyroid medication</th>\n",
       "      <th>referral source</th>\n",
       "      <th>education-num</th>\n",
       "      <th>psych</th>\n",
       "      <th>occupation</th>\n",
       "      <th>TBG measured</th>\n",
       "      <th>TBG</th>\n",
       "      <th>pregnant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>Alfred Still</td>\n",
       "      <td>4175 Smith Keys\\r\\nNew Taylor, NH 39815</td>\n",
       "      <td>57.0</td>\n",
       "      <td>M</td>\n",
       "      <td>1960-11-02</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>82.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>other</td>\n",
       "      <td>13.0</td>\n",
       "      <td>f</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>Amelia Rodriguez</td>\n",
       "      <td>087 Gary Port\\r\\nWest Sarah, KY 66896</td>\n",
       "      <td>77.0</td>\n",
       "      <td>F</td>\n",
       "      <td>1941-03-17</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>White|United-States\\r\\nBachelors -- Widowed|Un...</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>SVI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>Sales</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>Angela Boyer</td>\n",
       "      <td>3750 Chen Groves\\r\\nPamelatown, ME 02894</td>\n",
       "      <td>75.0</td>\n",
       "      <td>F</td>\n",
       "      <td>1942-12-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>White|United-States\\r\\nHS-grad -- Divorced|Own...</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>SVI</td>\n",
       "      <td>9.0</td>\n",
       "      <td>f</td>\n",
       "      <td>Priv-house-serv</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>Anna Garcia</td>\n",
       "      <td>71052 Annette Roads\\r\\nChristinechester, MT 16249</td>\n",
       "      <td>65.0</td>\n",
       "      <td>F</td>\n",
       "      <td>1953-05-06</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>White|United-States\\r\\nHS-grad -- Never-marrie...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>f</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>Annette Hunt</td>\n",
       "      <td>USNV Lamb\\r\\nFPO AA 85130</td>\n",
       "      <td>33.0</td>\n",
       "      <td>F</td>\n",
       "      <td>1984-12-08</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>White|United-States\\r\\nSome-college -- Married...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>f</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see there are duplicates with same name and address, but they are even not representing different medical records (measurements are the same). In some attributes, one of duplicates has value and in the other one is this value missing. That means we need to merge those records before droping duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "duplicated = df_train[df_train.duplicated(['name', 'address'], keep=false)]\n",
    "duplicate_names = df_train[df_train.duplicated(['name', 'address'], keep='first')].name.values\n",
    "\n",
    "for name in duplicate_names:\n",
    "    duplicates = duplicated[duplicated['name'] == name]\n",
    "    \n",
    "\n",
    "df_train =  df_train[~df_train['name'].isin(duplicate_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def func(vstup):\n",
    "    return reduce(lambda x,y: x if not pd.isna(x) else y, vstup)\n",
    "\n",
    "\n",
    "def deduplicate(df,columns = []):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    deduplicated = df_copy[df_copy.duplicated(subset=columns, keep=False)].groupby(columns).agg(func).reset_index()\n",
    "    \n",
    "    df_copy.drop_duplicates(subset=columns, keep=False, inplace=True)\n",
    "    \n",
    "    return pd.concat([df_copy,deduplicated], sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func({1, None, None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class MergeRemoveDuplicates(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def func(vstup):\n",
    "        return reduce(lambda x,y: x if not pd.isna(x) else y, vstup)\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        duplicated = df[df.duplicated(['name', 'address'], keep=false)]\n",
    "        duplicate_names = df[df.duplicated(['name', 'address'], keep='first')].name.values\n",
    "        df =  df[~df['name'].isin(duplicate_names)]\n",
    "\n",
    "        return df.append(duplicated.groupby(['name', 'address']).agg(func).reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Note:** This class will be used for preprocessing in **Pipelines**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Drop rows with missing values in predicted attribute\n",
    "\n",
    "Rows where even value of predicted attribute is missing, will not help classifying in *supervised learning*. In this case, those values would be dropped. Let's check records with missing values for **class** attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['class'].isnull()][['name', 'class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To make this operation reusable, it is better to write custom pipeline with column as parameter, so every row with missing values in this column will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class DropRowsNanColumn(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df = df[pd.notnull(df[self.column])]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Numerical missing values unifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class UnifyNumMissing(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df.loc[df[self.column] == '?', self.column] = np.NaN\n",
    "        df[self.column] = pd.to_numeric(df[self.column])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Boolean unifying\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class UnifyBoolean(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df_copy = df.copy()\n",
    "        df_copy[self.column] = df_copy[self.column].map(lambda x: str(x).lower().startswith('t'), na_action='ignore')\n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Drop useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class DropColumn(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "        \n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, **transform_params):\n",
    "        df = df.drop([self.column], axis=1)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Expanding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class ExpandColumn(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df['bred'] = df['personal_info'].str.extract('(^[^|]+)', expand=False).str.strip().str.lower()\n",
    "        df['origin'] = df['personal_info'].str.extract('[|](.*)\\r', expand=False).str.strip().str.lower()\n",
    "        df['study'] = df['personal_info'].str.extract('[\\n](.*)--', expand=False).str.strip().str.lower()\n",
    "        df['status1'] = df['personal_info'].str.extract('--(.*)[|]', expand=False).str.strip().str.lower()\n",
    "        df['status2'] = df['personal_info'].str.extract('--.*[|](.*)', expand=False).str.strip().str.lower()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "repair_ppl = Pipeline([\n",
    "                ('ub01', UnifyBoolean('query hyperthyroid')),\n",
    "                ('ub02', UnifyBoolean('T4U measured')),\n",
    "                ('ub03', UnifyBoolean('on thyroxine')),\n",
    "                ('ub04', UnifyBoolean('FTI measured')),\n",
    "                ('ub05', UnifyBoolean('lithium')),\n",
    "                ('ub06', UnifyBoolean('TT4 measured')),\n",
    "                ('ub07', UnifyBoolean('query hypothyroid')),\n",
    "                ('ub08', UnifyBoolean('query on thyroxine')),\n",
    "                ('ub09', UnifyBoolean('tumor')),\n",
    "                ('ub10', UnifyBoolean('T3 measured')),\n",
    "                ('ub11', UnifyBoolean('sick')),\n",
    "                ('ub12', UnifyBoolean('thyroid surgery')),\n",
    "                ('ub13', UnifyBoolean('I131 treatment')),\n",
    "                ('ub14', UnifyBoolean('goitre')),\n",
    "                ('ub15', UnifyBoolean('TSH measured')),\n",
    "                ('ub16', UnifyBoolean('on antithyroid medication')),\n",
    "                ('ub17', UnifyBoolean('psych')),\n",
    "                ('ub18', UnifyBoolean('TBG measured')),\n",
    "                ('ub19', UnifyBoolean('pregnant')),\n",
    "                ('ub20', UnifyBoolean('hypopituitary')),\n",
    "\n",
    "                ('dc1', DropColumn('TBG measured')),\n",
    "\n",
    "                ('ec1', ExpandColumn()),\n",
    "                ('unmv1', UnifyNumMissing('FTI')),\n",
    "\n",
    "               ('drop_class', DropRowsNanColumn('class')),\n",
    "                ('test',MergeRemoveDuplicates())\n",
    "\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "model = repair_ppl.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext/sage/sage-8.4_1804/local/lib/python2.7/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "transformed = model.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Terry Terry', 'Edith Boudreaux', 'Janet Washington', ...,\n",
       "       'Kenneth Smith', 'Antoinette Spencer', 'Sara Mcpherson'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Normalize and remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from scipy.stats import boxcox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Normalize numerical attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class Normalizer(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        _, self.lmbda = boxcox(df[self.column]+2)\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df_copy = df.copy()\n",
    "        df_copy[self.column] = boxcox(df_copy[self.column]+2, lmbda=attr)\n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class OutliersRemover(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        self.quantile_05 = df[self.column].quantile(.05)\n",
    "        self.quantiles_95 = df[self.column].quantile(.95)\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df.loc[df[self.column] > self.quantile_05, self.column] = self.quantile_05\n",
    "        df.loc[df[self.column] > self.quantile_95, self.column] = self.quantile_95\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need more than 0 values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-530edbecffa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m normalize_ppl = Pipeline([\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               ])\n",
      "\u001b[0;32m/ext/sage/sage-8.4_1804/local/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, steps, memory)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext/sage/sage-8.4_1804/local/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36m_validate_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# validate names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need more than 0 values to unpack"
     ]
    }
   ],
   "source": [
    "normalize_ppl = Pipeline([\n",
    "\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "model = normalize_ppl.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "transformed = model.transform(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Filling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Fill numerical with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class NumMedianFiller(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        self.median = dataframe[self.column].median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df.loc[df[self.column].isnull(), self.column] = self.median\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Fill numerical with Linear Regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class NumModelFiller(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df_copy = df.copy()\n",
    "        df_copy.loc[df_copy[self.column] == '?'] = np.NaN\n",
    "        df_copy[self.column] = pd.to_numeric(df_copy[self.column])\n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Fill categorical with most frequent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class CategoricalMostFrequentFiller(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "        \n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        self.most_frequent = df[self.column].value_counts().index[0]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, **transform_params):\n",
    "        df.loc[df[self.column].isnull(), self.column] = self.most_frequent\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Fill categorical with k-NN (k-nearest neighbours) algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class CategoricalModelFiller(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        df_copy = df.copy()\n",
    "        df_copy.loc[df_copy[self.column] == '?'] = np.NaN\n",
    "        df_copy[self.column] = pd.to_numeric(df_copy[self.column])\n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "fill_ppl = Pipeline([\n",
    "                ('nieco', CategoricalModelFiller('column'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "model = fill_ppl.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "transformed = model.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "tmp = transformed[~transformed['FTI'].isnull()]\n",
    "columns = ['TT4', 'T4U','capital-loss', 'capital-gain', 'TSH', 'T3', 'fnlwgt', 'hours-per-week', 'education-num']\n",
    "\n",
    "for column in columns:\n",
    "    tmp.dropna(subset=[column], inplace=True)\n",
    "    \n",
    "X = tmp[columns]\n",
    "y = tmp['FTI']\n",
    "\n",
    "X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "train_preds = model.predict(X_test)\n",
    "metrics.mean_absolute_error(y_test, train_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "len(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "pca = TruncatedSVD(n_components=1)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "train1 = pca.transform(X_train)\n",
    "test1 = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "train_preds = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plt.scatter(test1, y_test, color = 'red')\n",
    "plt.plot(train1, train_preds, color = 'blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath (stable)",
   "language": "sagemath",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}